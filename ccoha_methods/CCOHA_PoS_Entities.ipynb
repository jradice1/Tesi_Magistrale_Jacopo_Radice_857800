{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CCOHA - PoS - Entities.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcwg0rdf3vIq"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "root_dir = \"/content/drive/My Drive/\"\n",
        "base_dir = root_dir + 'Tesi/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-ryApQz5qJt"
      },
      "source": [
        "wiki_dir = root_dir + 'Tesi/Wiki50/Wikipedia/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iloNew1dXgn_"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6BguXpyUJy1"
      },
      "source": [
        "import nltk.tokenize.mwe\n",
        "from nltk import tokenize\n",
        "from nltk import pos_tag, word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z7pfODDEL8W"
      },
      "source": [
        "!pip install stanza"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YRI34h5KHQ5"
      },
      "source": [
        "import stanza\n",
        "stanza.download('en') # download English model\n",
        "nlp = stanza.Pipeline('en') # initialize English neural pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z253tCUPxHbK"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsyGcWNgta1j"
      },
      "source": [
        "from itertools import chain\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rlXt9elvgdL"
      },
      "source": [
        "from tqdm import tqdm\n",
        "from time import sleep\n",
        "tqdm.pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dHbwGGrBI2M"
      },
      "source": [
        "DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnS0GC62Dga7"
      },
      "source": [
        "import pandas as pd \n",
        "\n",
        "#df = pd.read_csv(base_dir + 'downstream/wikicorp_preprocessed.csv')\n",
        "\n",
        "df = pd.read_csv(base_dir + 'downstream/ccoha2_preprocessed.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afmlCV08ByLb"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk9_LpEQ6QOC"
      },
      "source": [
        "WIKIPEDIA DUMP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwJc9tw-fUqk"
      },
      "source": [
        "with open(wiki_dir + 'wikipedia_list_preprocessed.txt') as f:\n",
        "    wikipedia_list = f.read().splitlines() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdYnlPRxfM14"
      },
      "source": [
        "ENTITY EXTRACTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDijMCRaotS1"
      },
      "source": [
        "df[\"entities\"]=\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Wj9aSorJ4G0"
      },
      "source": [
        "def entities_extraction(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    entities= [ent.text for sentence in doc.sentences for ent in sentence.entities if ent.type in {\"PERSON\", \"ORG\", \"GPE\", \"NORP\", \"FAC\", \"LOC\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\", \"LAW\", \"LANGUAGE\", \"MISC\"}]\n",
        "\n",
        "    return entities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23uB7mdaNoHR"
      },
      "source": [
        "#df[\"entities\"] = df['text'].apply(lambda x: entities_extraction(x))\n",
        "\n",
        "df[\"entities\"] = df['text'].progress_apply(lambda x: entities_extraction(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dwx6LHM3o5aI"
      },
      "source": [
        "entities_without_unigrams = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlSzYJZVo7AK"
      },
      "source": [
        "for i in range(len(df)):\n",
        "  for kw in df['entities'][i]:\n",
        "    if(' ' in kw):\n",
        "        kw = kw.lower()\n",
        "        entities_without_unigrams.append(kw)\n",
        "  df['entities'][i]=entities_without_unigrams\n",
        "  entities_without_unigrams = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhRdoP1gpfCJ"
      },
      "source": [
        "entities_without_article = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlwEc-hVpYSe"
      },
      "source": [
        "for i in range(len(df)):\n",
        "  for kw in df['entities'][i]:\n",
        "    if('the' in kw[:4]):\n",
        "        kw = re.sub(\"the \", \"\", kw)\n",
        "        entities_without_article.append(kw)\n",
        "    else:\n",
        "        entities_without_article.append(kw)\n",
        "  df['entities'][i]=entities_without_article\n",
        "  entities_without_article = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeF_wT8QPpBz"
      },
      "source": [
        "for s in range(len(df)):  \n",
        "  for i in range(len(df['entities'][s])):\n",
        "    df['entities'][s][i] = df['entities'][s][i].split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZUdGvb9PpB0"
      },
      "source": [
        "for s in range(len(df)):  \n",
        "  for i in range(len(df['entities'][s])-1,-1,-1):\n",
        "    if (df['entities'][s][i][0] == df['entities'][s][i-1][-1]):\n",
        "      df['entities'][s][i-1].remove(df['entities'][s][i-1][-1])\n",
        "      df['entities'][s][i] = df['entities'][s][i-1] + df['entities'][s][i]\n",
        "      df['entities'][s].remove(df['entities'][s][i-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60yf9MjjPpB1"
      },
      "source": [
        "for s in range(len(df)):   \n",
        "  for i in range(len(df['entities'][s])):\n",
        "    df['entities'][s][i] = ' '.join(df['entities'][s][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDV1UjGPoxVJ"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjuJsab6Cuij"
      },
      "source": [
        "CON DATASET GIÃ€ MODIFICATO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAmJOvD1Cuil"
      },
      "source": [
        "import pandas as pd \n",
        "\n",
        "df = pd.read_csv(base_dir + 'downstream/checkpoints/echeckpoint2_PoS-Ent.csv', converters={'entities': eval})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjbdASk8Cuim"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV97WSIwKYDs"
      },
      "source": [
        "POS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv8Ban2_QE-T"
      },
      "source": [
        "df[\"pos\"]=\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gQUv9D_UThK"
      },
      "source": [
        "def extract_phrases(my_tree, phrase):\n",
        "   my_phrases = []\n",
        "   if my_tree.label() == phrase:\n",
        "      my_phrases.append(my_tree.copy(True))\n",
        "\n",
        "   for child in my_tree:\n",
        "       if type(child) is nltk.Tree:\n",
        "            list_of_phrases = extract_phrases(child, phrase)\n",
        "            if len(list_of_phrases) > 0:\n",
        "                my_phrases.extend(list_of_phrases)\n",
        "\n",
        "   return my_phrases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF-Sq7bUbtQJ"
      },
      "source": [
        "grammar = \"WP: {<NN><NN.?><NN.?><NN|NNS>|<NN|NNS><IN><DT>?<NN>|<NN><POS><NN|NNS>|<JJ><NN|NNS>+|<NN|NNS><IN><JJ><NN|NNS>|<IN><NN|NNS>|<NN|NNS><CC><NN|NNS>|<NN|NNS><NN>?<NN>?<NN|NNS>|<JJ>+<NN|NNS>}\"\n",
        "\n",
        "cp = nltk.RegexpParser(grammar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ymb3HwaHaOte"
      },
      "source": [
        "def pos_extraction(x):\n",
        "    pos_list=[]\n",
        "\n",
        "    sentence = pos_tag(tokenize.word_tokenize(x))\n",
        "    tree = cp.parse(sentence)\n",
        "    list_of_noun_phrases = extract_phrases(tree, 'WP')\n",
        "    for phrase in list_of_noun_phrases:\n",
        "      pos_list.append(\" \".join([x[0] for x in phrase.leaves()]))\n",
        "\n",
        "    text=pos_list\n",
        "    \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d62r7Z3Ja5qZ"
      },
      "source": [
        "df[\"pos\"] = df['text'].progress_apply(lambda x: pos_extraction(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "42FfksXA1qtG"
      },
      "source": [
        "pos_without_unigrams = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjmz_Yb51s4L"
      },
      "source": [
        "for i in range(len(df)):\n",
        "  for kw in df['pos'][i]:\n",
        "    if(' ' in kw):\n",
        "        kw = kw.lower()\n",
        "        pos_without_unigrams.append(kw)\n",
        "  df['pos'][i]=pos_without_unigrams\n",
        "  pos_without_unigrams = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhQs6tlNeG6G"
      },
      "source": [
        "for s in range(len(df)):  \n",
        "  for i in range(len(df['pos'][s])):\n",
        "    df['pos'][s][i] = df['pos'][s][i].split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYF6NqSVCeh5"
      },
      "source": [
        "for s in range(len(df)):  \n",
        "  for i in range(len(df['pos'][s])-1,-1,-1):\n",
        "    if (df['pos'][s][i][0] == df['pos'][s][i-1][-1]):\n",
        "      df['pos'][s][i-1].remove(df['pos'][s][i-1][-1])\n",
        "      df['pos'][s][i] = df['pos'][s][i-1] + df['pos'][s][i]\n",
        "      df['pos'][s].remove(df['pos'][s][i-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMbMirjiCiHt"
      },
      "source": [
        "for s in range(len(df)):   \n",
        "  for i in range(len(df['pos'][s])):\n",
        "    df['pos'][s][i] = ' '.join(df['pos'][s][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr_ahVM0SO-K"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w51KYc5C09k_"
      },
      "source": [
        "PREPROCESS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wB_CEVIvz79q"
      },
      "source": [
        "''''\n",
        "for i in range(len(df)):\n",
        "  df['pos'][i] = list(dict.fromkeys(df['pos'][i]))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjUv54NcsbqC"
      },
      "source": [
        "'''\n",
        "for i in range(len(df)):\n",
        "  df['entities'][i] = list(dict.fromkeys(df['entities'][i]))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56nLrHbevAbr"
      },
      "source": [
        "df[\"united\"] = df[\"pos\"] + df[\"entities\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18oXehRbwN8K"
      },
      "source": [
        "'''\n",
        "for i in range(len(df)):\n",
        "  df['united'][i] = list(dict.fromkeys(df['united'][i]))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g85gzFNhDo25"
      },
      "source": [
        "CON DATASET GIÃ€ MODIFICATO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V03Og4YDsXi"
      },
      "source": [
        "'''\n",
        "import pandas as pd \n",
        " \n",
        "df = pd.read_csv(base_dir + 'checkpointText_PoS-Ent.csv', converters={'united': eval})\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIIk6C32yPA2"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CH-K3eT-Qsc"
      },
      "source": [
        "WIKIPEDIA COMPARISON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbiKfy_sJcIf"
      },
      "source": [
        "df['check'] = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5QYeyHuyyYW"
      },
      "source": [
        "wikipedia_list = set(wikipedia_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEFqenZ-y2c1"
      },
      "source": [
        "df['check'] = [set(df['united'][i]).intersection(wikipedia_list) for i in tqdm(range(len(df)))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3qNoZpOkAAr"
      },
      "source": [
        "MWE THRESHOLD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioOO5nk0kCOT"
      },
      "source": [
        "mwe_counter = Counter(chain.from_iterable(df['check'])) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qK8ZgW4kJQi"
      },
      "source": [
        "df['final'] = \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POzDZZIdkLJF"
      },
      "source": [
        "df['final'] = [([j for j in i if mwe_counter[j] >= 5]) for i in df['check']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhfhgzzENDlL"
      },
      "source": [
        "MWE SUBSTITUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7bn8tPpNHfY"
      },
      "source": [
        "for i in range(len(df)):\n",
        "  df['text'][i] = re.sub('('+'|'.join('\\\\b'+re.escape(g)+'\\\\b' for g in df['final'][i])+')', lambda m: m.group(0).replace(' ', '_'), df['text'][i], flags=re.I)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa21iVNQPW9M"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlMgBUDPQ2Ju"
      },
      "source": [
        "compression_opts = dict(method='zip', archive_name='newCCOHA2_PoS-Ent.csv')\n",
        "\n",
        "df['text'].to_csv('newCCOHA2_PoS-Ent.zip', index=False, compression=compression_opts)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}